diff --git a/src/main.py b/src/main.py
index 06b9326..cf368f9 100644
--- a/src/main.py
+++ b/src/main.py
@@ -1,17 +1,12 @@
-import torch
-import torchvision.transforms as transforms
-
-# from load3dData import load_data
 from utils.dataloading_train import load_training_data
-from train import train_loop_class, train_lightning
+from train import train_lightning
 from utils.dataloading_test import load_testing_data
 from test import (
-    test_model,
-    calculateAccuracy,
     test_lightning_model,
     calculate_accuracy_lightning,
 )
 
+
 if __name__ == "__main__":
     import argparse
 
@@ -43,6 +38,7 @@ if __name__ == "__main__":
     parser.add_argument(
         "--model_path", type=str, help="Set the path of the model to be tested"
     )
+    parser.add_argument("--exp_name", type=str, help="Experiment name")
     parser.add_argument("--num_classes", default=1, type=int)
     args = parser.parse_args()
 
@@ -55,20 +51,18 @@ if __name__ == "__main__":
             # Preprocess training data. When first time called, data is preprocessed and saved to "my_training_data".
             # When this folder exists, data is loaded from it directly.
             train_loader, val_loader = load_training_data(args)
-            print("Number of samples in datasets:")
-            print(" training: " + str(len(train_loader.dataset)))
-            print(" validation: " + str(len(val_loader.dataset)))
-            print("Shape of data:")
-            print(" image: " + str(next(iter(train_loader))[0].shape))
-            print(
-                " target malignancy label: "
-                + str(next(iter(train_loader))[1].shape)
-            )
+
             # Train model and saves best performing model at model_path.
             # model_path = train_loop_class(train_loader, val_loader, args)
-            model_path = train_lightning(train_loader, val_loader, args)
-            print("Model saved at: " + str(model_path))
-            if args.test and args.testing_data_path is not None:
+            model_path, proc_rank = train_lightning(
+                train_loader, val_loader, args
+            )
+
+            if (
+                args.test
+                and args.testing_data_path is not None
+                and proc_rank == 0
+            ):
                 # Preprocess testing data. When first time called, data is preprocessed and saved to "my_testing_data".
                 # When this folder exists, data is loaded from it directly.
                 test_loader = load_testing_data(args)
@@ -77,7 +71,7 @@ if __name__ == "__main__":
                 args.model_path = model_path
                 # Testing data is being predicted and predictions are being saved in folder "testing_data_prediction_classification".
                 test_lightning_model(test_loader, args)
-                if not args.testing_data_solution_path == None:
+                if args.testing_data_solution_path is not None:
                     # Accuracy metric is being calculated between data in folder args.testing_data_solution_path and "testing_data_prediction_classification".
                     (
                         test_acc,
@@ -96,19 +90,19 @@ if __name__ == "__main__":
                 'Please specify the path to the training data by setting the parameter --training_data_path="path_to_training data"'
             )
     elif args.test:
-        if args.model_path == None:
+        if args.model_path is None:
             raise TypeError(
                 'Please specify the path to model by setting the parameter --model_path="path_to_model"'
             )
         else:
-            if not args.testing_data_path == None:
+            if args.testing_data_path is not None:
                 # Preprocess testing data. When first time called, data is preprocessed and saved to "my_testing_data"; this takes a considerably amount of time.
                 # When this folder exists, data is loaded from it directly.
                 test_loader = load_testing_data(args)
                 # Testing data is being predicted and predictions are being saved in folder "testing_data_prediction_classification".
                 # test_model(test_loader, args)
                 test_lightning_model(test_loader, args)
-                if not args.testing_data_solution_path == None:
+                if args.testing_data_solution_path is not None:
                     # Accuracy metric is being calculated between data in folder args.testing_data_solution_path and "testing_data_prediction_classification".
                     (
                         test_acc,
diff --git a/src/models/lightning_module.py b/src/models/lightning_module.py
index b5f5a6f..5b27b73 100644
--- a/src/models/lightning_module.py
+++ b/src/models/lightning_module.py
@@ -1,9 +1,6 @@
 import lightning.pytorch as pl
 import torch
-from monai.data import decollate_batch
-from monai.inferers import SliceInferer
 from monai.optimizers import Novograd
-from monai.transforms import Compose
 from torchmetrics import Accuracy, AUROC, F1Score, Precision, Recall
 from efficientnet_pytorch_3d import EfficientNet3D
 from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss
@@ -19,16 +16,13 @@ class LightningModel(pl.LightningModule):
     ):
         super().__init__()
 
-        # self.model = EfficientNet3D.from_name(
-        #     "efficientnet-b0",
-        #     override_params={"num_classes": num_classes},
-        #     in_channels=in_channels,
-        # )
         self.model = threeDClassModel(
             input_size=in_channels, num_classes=num_classes
         )
 
-        self.loss = BCEWithLogitsLoss()
+        self.loss = (
+            BCEWithLogitsLoss() if num_classes == 1 else CrossEntropyLoss()
+        )
 
         self.accuracy = Accuracy("binary")
         self.auroc = AUROC(task="binary")
@@ -127,26 +121,6 @@ class LightningModel(pl.LightningModule):
         )
         return loss
 
-    # def predict_step(self, batch, batch_idx, dataloader_idx=None):
-    #     x = batch["image"]
-    #     inferer = SliceInferer(
-    #         roi_size=(self.in_shape[2], self.in_shape[3]),
-    #         spatial_dim=2,
-    #         progress=False,
-    #     )
-    #     y_hat = inferer(x, self.model)
-    #     # #### THIS DOES NOT WORK ON OPENNEURO
-    #     # THIS IS DUE TO SOME PROBLEMS WITH INVERSION OF
-    #     # AFFINE TRANSFORMS. THIS VERSION RUNS ON 3D VOLUMES ONLY FROM
-    #     # THE DATASET PROVIDED BY THE CHALLENGE.
-    #     # #####
-    #     batch_copied = batch.copy()
-    #     batch_copied["pred"] = y_hat
-    #     batch_copied = [
-    #         self.predict_transforms(i) for i in decollate_batch(batch_copied)
-    #     ]
-    #     return y_hat
-
     def configure_optimizers(self):
         optimizer = Novograd(
             self.parameters(), lr=self.lr, amsgrad=True, weight_decay=0.001
diff --git a/src/test.py b/src/test.py
index ebca06d..e70ada2 100644
--- a/src/test.py
+++ b/src/test.py
@@ -1,14 +1,7 @@
 import os
 import shutil
 import torch
-from sklearn.metrics import confusion_matrix
 import numpy as np
-from sklearn.metrics import (
-    roc_auc_score,
-    f1_score,
-    precision_score,
-    recall_score,
-)
 from models.lightning_module import LightningModel
 from torchmetrics import Accuracy, AUROC, F1Score, Precision, Recall
 
@@ -87,81 +80,3 @@ def calculate_accuracy_lightning(args):
     recall = Recall(task="binary").to("cuda")(pred, y)
 
     return acc, auc, f1s, precision, recall
-
-
-def test_model(dataloader, args):
-    model = torch.load(args.model_path)
-    model.cuda()
-    if os.path.isdir("testing_data_prediction_classification"):
-        shutil.rmtree("testing_data_prediction_classification")
-    os.makedirs("testing_data_prediction_classification")
-    test_fnc_final(model, dataloader)
-
-
-def test_fnc_final(test_model, data_loader):
-    test_model.eval()
-    with torch.no_grad():
-        for i, (x, scanIDs, nodIDs) in enumerate(data_loader):
-            x = x.to("cuda", dtype=torch.float)
-
-            pred_mal = test_model(x)
-            for samplei in range(len((scanIDs))):
-                if not os.path.isdir(
-                    "testing_data_prediction_classification/scan_"
-                    + str(int(scanIDs[samplei].item()))
-                ):
-                    os.makedirs(
-                        "testing_data_prediction_classification/scan_"
-                        + str(int(scanIDs[samplei].item()))
-                    )
-                np.savetxt(
-                    "testing_data_prediction_classification/scan_"
-                    + str(int(scanIDs[samplei].item()))
-                    + "/nodule_"
-                    + str(int(nodIDs[samplei].item()))
-                    + ".txt",
-                    pred_mal[samplei].cpu(),
-                    delimiter=",",
-                )
-
-
-def calculateAccuracy(args):
-    y_list = []
-    pred_list = []
-    for scan_file in os.listdir(args.testing_data_solution_path):
-        for nod_file in os.listdir(
-            args.testing_data_solution_path + "/" + scan_file
-        ):
-            y = np.loadtxt(
-                args.testing_data_solution_path
-                + "/"
-                + scan_file
-                + "/"
-                + nod_file,
-                delimiter=",",
-            )
-            pred = np.loadtxt(
-                "testing_data_prediction_classification/"
-                + scan_file
-                + "/"
-                + nod_file,
-                delimiter=",",
-            )
-            y_list.append(y)
-            pred_list.append(pred)
-
-    y = np.asarray(y_list)
-    pred = np.asarray(pred_list)
-
-    mal_confusion_matrix = confusion_matrix(
-        np.argmax(pred, axis=1), np.argmax(y, axis=1), labels=[0, 1]
-    )
-    mal_correct = sum(np.diagonal(mal_confusion_matrix, offset=0))
-    acc = mal_correct / y.shape[0]
-
-    auc = roc_auc_score(y, pred)
-    f1s = f1_score(np.argmax(y, axis=1), np.argmax(pred, axis=1))
-    precision = precision_score(np.argmax(y, axis=1), np.argmax(pred, axis=1))
-    recall = recall_score(np.argmax(y, axis=1), np.argmax(pred, axis=1))
-
-    return acc, auc, f1s, precision, recall
diff --git a/src/train.py b/src/train.py
index 0521606..c9e5384 100644
--- a/src/train.py
+++ b/src/train.py
@@ -1,16 +1,9 @@
 import torch
-from torch import nn
-import numpy as np
-from torchmetrics import AUROC, F1Score, Precision, Recall
-import datetime
-import matplotlib.pyplot as plt
-from models.model_original import threeDClassModel
-from efficientnet_pytorch_3d import EfficientNet3D
-from torchsummary import summary
-from monai.optimizers import Novograd
 from models.lightning_module import LightningModel
 import lightning.pytorch as pl
-from lightning.pytorch.callbacks import EarlyStopping
+import wandb
+from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint
+from lightning.pytorch.loggers import WandbLogger
 from lightning.pytorch.strategies.ddp import DDPStrategy
 
 torch.set_float32_matmul_precision("medium")
@@ -29,14 +22,26 @@ def train_lightning(train_loader, val_loader, args):
         verbose=True,
         mode="min",
     )
-    model_checkpoint_callback = pl.callbacks.ModelCheckpoint(
+    model_checkpoint_callback = ModelCheckpoint(
         save_top_k=1,
         mode="min",
         monitor="val_loss",
         dirpath="checkpoints/",
         filename="best_model",
     )
-    strategy = pl.strategies.DDPStrategy(
+    # wandb.init(
+    #     project="efficient-classification",
+    #     group="general_tests",
+    #     job_type="batch_size_tests",
+    #     name=args.exp_name,
+    # )
+    # wandb_logger = WandbLogger(
+    #     project="efficient-classification",
+    #     group="general_tests",
+    #     job_type="batch_size_tests",
+    #     name=args.exp_name,
+    # )
+    strategy = DDPStrategy(
         find_unused_parameters=False,
         static_graph=True,
     )
@@ -45,191 +50,16 @@ def train_lightning(train_loader, val_loader, args):
         devices="auto",
         precision="bf16-mixed",
         strategy=strategy,
+        log_every_n_steps=1,
         max_epochs=args.epochs,
+        logger=WandbLogger(
+            project="efficient-classification",
+            group="general_tests",
+            job_type="batch_size_tests",
+            name=args.exp_name,
+        ),
         callbacks=[early_stop_callback, model_checkpoint_callback],
     )
     trainer.fit(model, train_loader, val_loader)
-    return model_checkpoint_callback.best_model_path
-
-
-def train_loop_class(train_loader, val_loader, args):
-    input_channels = next(iter(train_loader))[0].shape[1]
-    num_classes = next(iter(train_loader))[1].shape[-1]
-    # model = threeDClassModel(
-    #     input_size=input_channels, num_classes=num_classes
-    # )
-    model = EfficientNet3D.from_name(
-        "efficientnet-b0",
-        override_params={"num_classes": num_classes},
-        in_channels=input_channels,
-    )
-    model = model.cuda()
-
-    # optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)  # 0.001
-    optimizer = Novograd(model.parameters(), lr=args.lr)
-    datestr = str(datetime.datetime.now())
-    print("this run has datestr " + datestr)
-    tr_accs, tr_losses, tr_aucs, tr_f1ss, tr_precisions, tr_recalls = (
-        [],
-        [],
-        [],
-        [],
-        [],
-        [],
-    )
-    val_accs, val_aucs, val_f1ss, val_precisions, val_recalls = (
-        [],
-        [],
-        [],
-        [],
-        [],
-    )
-    best_val = 0.0
-    for ep in range(args.epochs):
-        print("Epoch " + str(ep))
-        print("Training")
-        (
-            model,
-            tr_acc,
-            tr_loss,
-            tr_auc,
-            tr_f1s,
-            tr_precision,
-            tr_recall,
-        ) = train_fnc(model, train_loader, optim=optimizer)
-        tr_accs.append(tr_acc)
-        tr_losses.append(tr_loss.cpu().detach().numpy())
-        tr_aucs.append(tr_auc.cpu().detach().numpy())
-        tr_f1ss.append(tr_f1s.cpu().detach().numpy())
-        tr_precisions.append(tr_precision.cpu().detach().numpy())
-        tr_recalls.append(tr_recall.cpu().detach().numpy())
-        val_acc, val_auc, val_f1s, val_precision, val_recall = val_fnc(
-            model, val_loader
-        )
-        val_accs.append(val_acc)
-        val_aucs.append(val_auc.cpu().detach().numpy())
-        val_f1ss.append(val_f1s.cpu().detach().numpy())
-        val_precisions.append(val_precision.cpu().detach().numpy())
-        val_recalls.append(val_recall.cpu().detach().numpy())
-        if val_f1s.cpu().detach().numpy() > best_val:
-            print("save new best model")
-            torch.save(model, str(datestr) + ".pth")
-            best_val = val_f1s.cpu().detach().numpy()
-    # plt.subplot(2, 6, 1)
-    # plt.plot(np.arange(args.epochs), tr_losses)
-    # plt.title("Train Loss")
-    # plt.subplot(2, 6, 2)
-    # plt.plot(np.arange(args.epochs), tr_accs)
-    # plt.title("Train Accuracy")
-    # plt.subplot(2, 6, 3)
-    # plt.plot(np.arange(args.epochs), tr_aucs)
-    # plt.title("Train AUC")
-    # plt.subplot(2, 6, 4)
-    # plt.plot(np.arange(args.epochs), tr_f1ss)
-    # plt.title("Train F1")
-    # plt.subplot(2, 6, 5)
-    # plt.plot(np.arange(args.epochs), tr_precisions)
-    # plt.title("Train Precision")
-    # plt.subplot(2, 6, 6)
-    # plt.plot(np.arange(args.epochs), tr_recalls)
-    # plt.title("Train Recall")
-    #
-    # plt.subplot(2, 6, 8)
-    # plt.plot(np.arange(args.epochs), val_accs)
-    # plt.title("Val Accuracy")
-    # plt.subplot(2, 6, 9)
-    # plt.plot(np.arange(args.epochs), val_aucs)
-    # plt.title("Val AUC")
-    # plt.subplot(2, 6, 10)
-    # plt.plot(np.arange(args.epochs), val_f1ss)
-    # plt.title("Val F1")
-    # plt.subplot(2, 6, 11)
-    # plt.plot(np.arange(args.epochs), val_precisions)
-    # plt.title("Val Precision")
-    # plt.subplot(2, 6, 12)
-    # plt.plot(np.arange(args.epochs), val_recalls)
-    # plt.title("Val Recall")
-    # plt.show()
-    return str(datestr) + ".pth"
-
-
-def loss_fcn(gt, pred):
-    L_pred = nn.CrossEntropyLoss()(torch.squeeze(pred, dim=-1), gt)
-    return L_pred
-
-
-def train_fnc(trainmodel, data_loader, optim):
-    trainmodel.train()
-    auc, f1s, precision, recall = [], [], [], []
-    correct_mal = 0
-    tr_loss = 0
-    for i, (x, y_mal) in enumerate(data_loader):
-        x, y_mal = x.to("cuda", dtype=torch.float), y_mal.to(
-            "cuda", dtype=torch.float
-        )
-        optim.zero_grad()
-        pred_mal = trainmodel(x)
-
-        loss = loss_fcn(y_mal, pred_mal)
-
-        loss.backward()
-        optim.step()
-        tr_loss += loss
-        auroc = AUROC(task="multiclass", num_classes=2).to("cuda")
-        auc.append(auroc(pred_mal, torch.argmax(y_mal, dim=1)))
-        f1score = F1Score(task="multiclass", num_classes=2).to("cuda")
-        f1s.append(f1score(pred_mal, torch.argmax(y_mal, dim=1)))
-        precisionscore = Precision(
-            task="multiclass", average="macro", num_classes=2
-        ).to("cuda")
-        precision.append(precisionscore(pred_mal, torch.argmax(y_mal, dim=1)))
-        recallscore = Recall(
-            task="multiclass", average="macro", num_classes=2
-        ).to("cuda")
-        recall.append(recallscore(pred_mal, torch.argmax(y_mal, dim=1)))
-
-    return (
-        trainmodel,
-        correct_mal / len(data_loader.dataset),
-        tr_loss / len(data_loader.dataset),
-        sum(auc) / len(auc),
-        sum(f1s) / len(f1s),
-        sum(precision) / len(precision),
-        sum(recall) / len(recall),
-    )
-
-
-def val_fnc(testmodel, data_loader):
-    testmodel.eval()
-    auc, f1s, precision, recall = [], [], [], []
-    correct_mal = 0
-    with torch.no_grad():
-        for i, (x, y_mal) in enumerate(data_loader):
-            x, y_mal = x.to("cuda", dtype=torch.float), y_mal.to(
-                "cuda", dtype=torch.float
-            )
-
-            pred_mal = testmodel(x)
-
-            auroc = AUROC(task="multiclass", num_classes=2).to("cuda")
-            auc.append(auroc(pred_mal, torch.argmax(y_mal, dim=1)))
-            f1score = F1Score(task="multiclass", num_classes=2).to("cuda")
-            f1s.append(f1score(pred_mal, torch.argmax(y_mal, dim=1)))
-            precisionscore = Precision(
-                task="multiclass", average="macro", num_classes=2
-            ).to("cuda")
-            precision.append(
-                precisionscore(pred_mal, torch.argmax(y_mal, dim=1))
-            )
-            recallscore = Recall(
-                task="multiclass", average="macro", num_classes=2
-            ).to("cuda")
-            recall.append(recallscore(pred_mal, torch.argmax(y_mal, dim=1)))
-
-    return (
-        correct_mal / len(data_loader.dataset),
-        sum(auc) / len(auc),
-        sum(f1s) / len(f1s),
-        sum(precision) / len(precision),
-        sum(recall) / len(recall),
-    )
+    process_rank = int(trainer.global_rank)
+    return model_checkpoint_callback.best_model_path, process_rank
diff --git a/src/utils/dataloading_train.py b/src/utils/dataloading_train.py
index d6ab94e..215a9cc 100644
--- a/src/utils/dataloading_train.py
+++ b/src/utils/dataloading_train.py
@@ -5,6 +5,13 @@ from torch.utils.data import Dataset
 from .loading_helper import generateMyTrainingData
 import h5py
 from sklearn.model_selection import train_test_split
+from torchio.transforms import (
+    Compose,
+    RandomFlip,
+    RandomAffine,
+    RandomElasticDeformation,
+)
+from torchvision.transforms import ToTensor
 
 
 def load_training_data(args):
@@ -16,7 +23,6 @@ def load_training_data(args):
     h5f = h5py.File("my_training_data/traindata.h5", "r")
     img_train = np.array(h5f["img"])
     tar_label_train = np.array(h5f["tar_label"])
-
     split_train, split_val = train_test_split(
         torch.arange(img_train.shape[0]),
         test_size=0.1,
@@ -26,16 +32,25 @@ def load_training_data(args):
     tar_train_labels = tar_label_train[split_train]
     val_imgs = np.transpose(img_train[split_val], (0, 3, 1, 2))
     tar_val_labels = tar_label_train[split_val]
-
-    fin_train_dataset = LIDCDataset(train_imgs, tar_train_labels)
+    T = Compose(
+        [
+            RandomFlip(axes=(0, 1, 2)),
+            RandomElasticDeformation(num_control_points=7, max_displacement=7),
+        ]
+    )
+    fin_train_dataset = LIDCDataset(
+        train_imgs,
+        tar_train_labels,
+    )  # transforms=T)
     fin_val_dataset = LIDCDataset(val_imgs, tar_val_labels)
     train_loader = torch.utils.data.DataLoader(
         fin_train_dataset,
         batch_size=args.batch_size,
         shuffle=True,
         pin_memory=True,
-        num_workers=3,
+        num_workers=6,
         prefetch_factor=4,
+        drop_last=False,
     )
     val_loader = torch.utils.data.DataLoader(
         fin_val_dataset,
@@ -43,6 +58,8 @@ def load_training_data(args):
         shuffle=False,
         pin_memory=True,
         num_workers=2,
+        prefetch_factor=4,
+        drop_last=False,
     )
 
     return train_loader, val_loader
